---
title: "Text as Data - Assignment 3"
author: "Jessica Higgins, Nikolina Klatt, Marco Schmildt, GÃ¼lce Sena Tuncer"
date: "`r format(Sys.time(), '%B %d, %Y | %H:%M:%S | %Z')`"
output:
  html_document:
    code_folding: show
    df_print: paged
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: TRUE
---

```{r, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      error = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)
```

```{r}
# PACKAGES
library(readr)
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(topicmodels)
library(stm)
library(plotQuote)
```

## R Markdown

We load our dataset:

```{r}
raw_df <-  read.csv2("data/corpus.csv", # File name or full path of the file
         header = TRUE,        # Whether to read the header or not
         sep = "\t",           # Separator of the values
         quote = "\"",         # Quoting character
         comment.char = "",    # Character of the comments or empty string 
         encoding = "unknown")

raw_df <- raw_df %>% 
  rename(number = X) 

raw_df <- raw_df %>% 
  arrange(year)

head(raw_df)

```


```{r}

raw_df <- raw_df %>% 
    mutate(text = strsplit(as.character(text), "(\\.\n|\\. \n|\\.  \n)")) %>% 
    unnest(text) %>% 
  group_by(country,year) %>% 
  mutate(paragraph_number = 1:n()) %>%
  filter(year > 2000) %>% 
  filter(text != "")%>%
  filter(text > 0 )

```



```{r}
processed <- textProcessor(raw_df$text, metadata = raw_df) 
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta


```


```{r}

plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 100))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)

```


```{r}

poliblogPrevFit <- stm(documents = out$documents, 
                       vocab = out$vocab, 
                       K = 20, 
                       prevalence = ~country + s(year), 
                       max.em.its = 75, 
                       data = out$meta, 
                       init.type = "Spectral")



saveRDS(poliblogPrevFit, file = "data/stm_poliblogPrevFit.rds")
poliblogPrevFit <- readRDS("data/stm_poliblogPrevFit.rds")
```


```{r}
poliblogSelect <- selectModel(out$documents, out$vocab, 
                              K = 20, 
                              prevalence =  ~country + s(year), 
                              max.em.its = 15, 
                              data = out$meta, 
                              runs = 20, 
                              seed = 8458159)

plotModels(poliblogSelect, pch = c(1, 2, 3, 4), legend.position = "bottomright")

saveRDS(poliblogSelect, file = "data/stm_poliblogSelect.rds")
poliblogSelect <- readRDS("data/stm_poliblogSelect.rds")

```



```{r}
plotModels(poliblogSelect, pch = c(1, 2, 3, 4), legend.position = "bottomright")

'selectedmodel <- poliblogSelect$runout[[3]]

```


```{r}

labelTopics(poliblogPrevFit, c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20))


```


```{r}

thoughts6 <- findThoughts(poliblogPrevFit, texts = shortdoc, n = 2, topics = 6)$docs[[1]]
thoughts18 <- findThoughts(poliblogPrevFit, texts = shortdoc, n = 2, topics = 18)$docs[[1]]
par(mfrow = c(1, 2), mar = c(0.5, 0.5, 1, 0.5))
plotQuote(thoughts6, width = 30, main = "Topic 6")
plotQuote(thoughts18, width = 30, main = "Topic 18")

```


```{r}
out$meta$country <- as.factor(out$meta$country)
prep <- estimateEffect(1:20 ~country + s(year), poliblogPrevFit, meta = out$meta, uncertainty = "Global")
summary(prep, topics = 1)

```


```{r}
plot(poliblogPrevFit, type = "summary", xlim = c(0, 0.3))

plot(poliblogSelect$runout[[2]], type = "summary", xlim = c(0, 0.3))

```

```{r}
plot(poliblogPrevFit, type = "hist", topics = sample(1:20, size = 9), main = "histogram of the topic shares within the documents")

plot(poliblogPrevFit, type = "hist", topics = sample(1:20, size = 9), main = "histogram of the topic shares within the documents")

plot(poliblogPrevFit, type = "hist", topics = (1:5), main = "histogram of the topic shares within the documents")
plot(poliblogPrevFit, type = "hist", topics = (6:10), main = "histogram of the topic shares within the documents")
plot(poliblogPrevFit, type = "hist", topics = (11:15), main = "histogram of the topic shares within the documents")
plot(poliblogPrevFit, type = "hist", topics = (16:20), main = "histogram of the topic shares within the documents")

```


```{r}
plot(poliblogPrevFit, type = "labels", topics = c(1, 2, 6, 15), main = "Topic terms")


```


```{r}
plot(poliblogPrevFit, type = "perspectives", topics = c(6,15), main = "Topic contrasts")

```


```{r}
modell.stm.labels <- labelTopics(poliblogPrevFit, 1:20)
out$meta$datum <- as.numeric(out$meta$year)
modell.stm.effekt <- estimateEffect(1:20 ~ country + s(year), poliblogPrevFit, meta = out$meta)



par(mfrow=c(3,3))
for (i in 1:9)
{
  plot(modell.stm.effekt, "year", method = "continuous", topics = i, main = paste0(modell.stm.labels$prob[i,1:3], collapse = ", "), ylab = "", printlegend = F)
}


saveRDS(modell.stm.effekt, file = "data/modell.stm.effekt.rds")
modell.stm.effekt <- readRDS("data/modell.stm.effekt.rds")

```


```{r}

save.image('stm_gadarian.RData')

library(stminsights)
run_stminsights()

```


```{r}


```


```{r}
plot(prep, "year", method = "continuous", topics = 15, model = z, printlegend = FALSE, xaxt = "n", xlab = "Year")
monthseq <- seq(from = as.Date("2000-01-01"), to = as.Date("2020-01-01"), by = "quarter")
monthnames <- quarters(monthseq)
axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)), labels = monthnames)

plot(prep, "year", method = "continuous", topics = 15, model = z, printlegend = FALSE, xaxt = "n", xlab = "Year")
axis(1, at = "year",labels = "year")

```


```{r}

mod.out.corr <- topicCorr(poliblogPrevFit)
plot(mod.out.corr)

```


```{r}
plot(poliblogPrevFit$convergence$bound, type = "l", ylab = "Approximate Objective", main = "Convergence")


```


```{r}

poliblogInteraction <- stm(out$text, out$vocab, 
                           K = 20, 
                           prevalence = ~country + s(year), 
                           max.em.its = 5, 
                           data = out$meta, 
                           init.type = "Spectral")

```


```{r}
prep <- estimateEffect(c(15) ~ country + s(year), poliblogInteraction, metadata = out$meta, uncertainty = "None")
plot(prep, covariate = "day", model = poliblogInteraction, + method = "continuous", xlab = "Days", moderator = "rating", moderator.value = "Liberal", linecol = "blue", ylim = c(0, 0.12), printlegend = FALSE)

plot(prep, covariate = "day", model = poliblogInteraction, method = "continuous", xlab = "Days", moderator = "rating", moderator.value = "Conservative", linecol = "red", add = TRUE, printlegend = FALSE)

legend(0, 0.06, c("Liberal", "Conservative"), lwd = 2, col = c("blue", "red"))


```


```{r}



```


```{r}



```



```{r}



```



```{r}



```



```{r}



```


```{r}



```
And create a corpus:

```{r}
# turn data into a corpus
corpus <- corpus(raw_df, text_field = "text")

# turn corpus into Document Feature Matrix
dfmat <- corpus %>% 
  tokens(., remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) %>%
  tokens_remove(pattern=stopwords("en")) %>% 
  dfm()

dfmat



dfmat <- quanteda::convert(dfmat, to = "topicmodels") 
#########################################################
  
  dfmlda


cdes <- dfm_trim(dfmat, min_docfreq = 2) %>%
   dfm_subset(ntoken(cdes) > 0)


#dtm <- DocumentTermMatrix(crude, sparse=TRUE)

raw.sum=apply(dfmat, 1, FUN= sum )

Lbdfmat=Lbdfmat[raw.sum!=0,]

lda_LB <- LDA(Lbdfmat, 20)

raw_df <- "1"

row_total = apply(text_dtm, 1, sum)
empty.rows <- text_dtm[rowTotals == 0, ]$dimnames[1][[1]]

raw.sum = apply(dfmat,1,FUN=sum) #sum by row each raw of the table
dfmat = dfmat_raw[raw.sum!=0,]

text_dtm <- DocumentTermMatrix(text_corpus_clean,control = list(tolower=TRUE,
                                                                removePunctuation = TRUE, 
                                                                removeNumbers= TRUE,
                                                                stopwords = TRUE,
                                                                sparse=TRUE))


```

LDA:

This is also interesting: https://towardsdatascience.com/why-to-use-seeded-topic-models-in-your-next-project-and-how-to-implement-them-in-r-8502d15d6e8d

```{r}
#we create a topic model using LDA
lda <- LDA(dfmat, k = 20)

#dimensions of beta attribute, where topic-term matrix is stored
print(dim(lda@beta))

#dimensions of gamma attribute, where document-term matrix is stored
print(dim(lda@gamma))

```

Using tidytext, we get the **topic-term probabilities**, displaying the top 8 terms in each topic.

```{r}
#we get our topic-term data
topic_term <- tidytext::tidy(lda, matrix="beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 8) %>%
  ungroup() %>%
  arrange(topic, -beta)
topic_term

```

```{r}
#top 8 term of each topic
wordplot <- topic_term %>%
  mutate(term = tidytext::reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  tidytext::scale_y_reordered() +
  labs(title = "Top 8 terms of each topic", subtitle = "UN General Assembly Debates, 1970-2020")

wordplot
```

Using tidytext, we get the **document-topic probabilities**, displaying composition of each document by topic.

```{r}
#we get our doc-topic data
doc_topics_01 <- tidytext::tidy(lda, matrix="gamma")

#doc_topics_02 <- merge(doc_topics_01,df,by="document",all=TRUE)
#doc_topics_02 <- select(doc_topics_02, -text)
#doc_topics_02
```

```{r}

as.data.frame(terms(lda, 10))
```


```{r}

LDA_result <- data.frame(Thema = topics(lda))
write.csv(LDA_result,"data/LDA_result_paragraphs.csv", row.names = FALSE)
LDA_result
```


```{r}

lda.themen.absaetze <- data.frame(raw_df, Thema = topics(lda)) %>%
  add_count(year, Thema) %>%
  group_by(year) %>% 
  mutate(Anteil = n/sum(n)) %>% 
  ungroup() %>% 
  mutate(Thema = paste0("Thema ", sprintf("%02d", Thema))) %>% 
  mutate(year = as_factor(year))
ggplot(lda.themen.absaetze, aes(year, Anteil, color = Thema, fill = Thema)) + geom_bar(stat="identity") + ggtitle("LDA-Topics ") + xlab("") + ylab("Themen-Anteil (%)") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}

saveRDS(lda, file = "data/lda_model_paragraphs.rds")

```

```{r}

lda <- readRDS("data/lda_model_paragraphs.rds")

```
